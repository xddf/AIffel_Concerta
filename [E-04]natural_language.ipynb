{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd16ef4",
   "metadata": {},
   "source": [
    "# 작사가 인공지능 만들기\n",
    "\n",
    "- 실습 노트와 프로젝트를 한 파일에 작성해뒀습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f303891c",
   "metadata": {},
   "source": [
    "## 0. 기본 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d13c67f",
   "metadata": {},
   "source": [
    "### import, 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c41851d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0067a172",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/Exploration/natural_language\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2420f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/aiffel/aiffel/Exploration/natural_language/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28edc4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[E-04]natural_language.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'lyricist',\n",
       " 'E-12-4.max-800x600.png']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "711443a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = path+'lyricist/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce596281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lyrics', 'shakespeare.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca259b6",
   "metadata": {},
   "source": [
    "### 예시 파일을 읽어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d94fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"
     ]
    }
   ],
   "source": [
    "file_path = file_dir+'shakespeare.txt'\n",
    "with open(file_path, \"r\") as f:          # 경로의 파일을 읽기모드로 열기. f로 명명\n",
    "    raw_corpus = f.read().splitlines()    # 라인 단위로 분할. 리스트 형태로 읽어온다?\n",
    "    \n",
    "print(raw_corpus[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d12a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before we proceed any further, hear me speak.\n",
      "Speak, speak.\n",
      "You are all resolved rather to die than to famish?\n"
     ]
    }
   ],
   "source": [
    "# enumerate(이뉴머레잍?)은 리스트의 요소와 인덱스 번호를 튜플 형태로 반환한다.\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db30c73",
   "metadata": {},
   "source": [
    "## 1.데이터 전처리 \n",
    "\n",
    "1. 정규표현식으로 코퍼스 정리\n",
    "2. 정리된 코퍼스를 텐서 데이터로 변환\n",
    "3. 텐서 데이터를 객체화\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7056a2",
   "metadata": {},
   "source": [
    "### 토큰화(Tokenize)\n",
    "\n",
    ": 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 합니다. 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 단위로 토큰을 정의합니다.\n",
    "\n",
    "- 코퍼스를 의미 단위로 나누는 작업\n",
    "\n",
    "\n",
    "- 보통 단어(+단어구, 유의미한 문자열)단위로 시행한다고 한다.\n",
    "\n",
    "\n",
    "- 정제되지 않은 코퍼스의 경우 문장 단위로 먼저 토큰화\n",
    "\n",
    "\n",
    "- 한국어는 토큰화가 난해하다.\n",
    "> - 교착어의 특성(어절 단위에 조사가 포함)\n",
    "> - 띄어쓰기가 잘 지켜지지 않음(어렵고, 안 지켜져도 변형이 심하지 않기 때문)\n",
    "\n",
    "\n",
    "- 1~2를 토큰화 과정으로 볼 수 있는 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968b28be",
   "metadata": {},
   "source": [
    "### 띄어쓰기를 기준으로 토큰화 한다면?\n",
    "\n",
    "#### 문제가 되는 케이스와 대첵\n",
    "- 문장부호\n",
    "> 부호 양 옆에 공백을 추가\n",
    "- 대/소문자 구분\n",
    "> 모든 문자를 소문자로 변환\n",
    "- 특수문자(ten-year-old 등)\n",
    "> 특수문자는 일단 다 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9aa77a",
   "metadata": {},
   "source": [
    "## 1.1. 정규표현식으로 코퍼스 정리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2fd8e",
   "metadata": {},
   "source": [
    "### 정규표현식(Regex)을 이용한 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc5d4072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2 \\1은 정규표현식 그룹1을 가리킨다.\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1487cb0",
   "metadata": {},
   "source": [
    "#### 자연어 처리 용어\n",
    "- Source Sentence(소스 문장) : 모델의 입력이 되는 문장. X_train에 해당\n",
    "- Target Sentence(타겟 문장) : 모델의 정답(출력) 문장. y_train에 해당"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc8361",
   "metadata": {},
   "source": [
    "### 정제된 문장을 리스트화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b3bfd18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> before we proceed any further , hear me speak . <end>',\n",
       " '<start> speak , speak . <end>',\n",
       " '<start> you are all resolved rather to die than to famish ? <end>',\n",
       " '<start> resolved . resolved . <end>',\n",
       " '<start> first , you know caius marcius is chief enemy to the people . <end>',\n",
       " '<start> we know t , we know t . <end>',\n",
       " '<start> let us kill him , and we ll have corn at our own price . <end>',\n",
       " '<start> is t a verdict ? <end>',\n",
       " '<start> no more talking on t let it be done away , away ! <end>',\n",
       " '<start> one word , good citizens . <end>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f50681c",
   "metadata": {},
   "source": [
    "## 1.2. 정리된 코퍼스를 텐서 데이터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3edb50",
   "metadata": {},
   "source": [
    "### 토큰화 코드 : 코퍼스를 텐서로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7233900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2  143   40 ...    0    0    0]\n",
      " [   2  110    4 ...    0    0    0]\n",
      " [   2   11   50 ...    0    0    0]\n",
      " ...\n",
      " [   2  149 4553 ...    0    0    0]\n",
      " [   2   34   71 ...    0    0    0]\n",
      " [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f6743d82490>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
    "# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n",
    "def tokenize(corpus):\n",
    "    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ec03d",
   "metadata": {},
   "source": [
    "### 토큰화 된 텐서 데이터의 인덱스를 확인해보자\n",
    "\n",
    "* 인덱스 0은 패딩 문자 \\<pad>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1312da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : .\n",
      "6 : the\n",
      "7 : and\n",
      "8 : i\n",
      "9 : to\n",
      "10 : of\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb99cf5",
   "metadata": {},
   "source": [
    "## 1.3. 텐서 데이터를 객체화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d949cd6",
   "metadata": {},
   "source": [
    "### 텐서를 소스와 타겟으로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b14119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n",
      "   0   0]\n",
      "[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n",
      "   0   0]\n"
     ]
    }
   ],
   "source": [
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다. (따라서 =1 입력)\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af2a16",
   "metadata": {},
   "source": [
    "### 데이터셋 객체 생성\n",
    "- 지금까지 실습에서 대부분 fit() 메소드를 사용해 npArray 데이터 셋을 생성, 모델에 제공\n",
    "> model.fit(X_train, y_train) 의 형태\n",
    "\n",
    "- 텐서플로우에서는 데이터를 텐서 타입으로 생성해서 학습시킨다.\n",
    "> tf.data.Detaset을 이용\n",
    "\n",
    "- 여기서는 이미 텐서 형태로 토큰화 했기 때문에 슬라이싱하여 객체를 생성할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bf39430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "# 데이터셋에 대해서는 아래 문서를 참고하세요\n",
    "# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0cee73",
   "metadata": {},
   "source": [
    "## 2. 학습 시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e651a",
   "metadata": {},
   "source": [
    "- 실습할 모델의 구조도\n",
    "![-](./E-12-4.max-800x600.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1497ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c60a447",
   "metadata": {},
   "source": [
    "- embedding_size : 단어가 추상적으로 표현되는 크기. 학습 관점에서 batch size로 볼 수 있다.\n",
    "\n",
    "\n",
    "- hidden_size : 모델의 일꾼(두뇌? 판단지표?)수 라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed802016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n",
       "array([[[-3.8165841e-04, -2.6243613e-05, -1.3113992e-04, ...,\n",
       "          3.4032954e-04,  2.2576917e-04, -3.0043759e-04],\n",
       "        [-4.2603182e-04,  1.3185639e-04, -1.9268265e-04, ...,\n",
       "          3.2288401e-04,  2.7062604e-04, -1.7772862e-04],\n",
       "        [-4.0846737e-04,  1.0529251e-04, -1.5365033e-04, ...,\n",
       "          2.2728206e-04, -2.0570704e-05, -3.3829674e-05],\n",
       "        ...,\n",
       "        [-2.7526354e-03,  2.6553178e-03,  2.3215306e-03, ...,\n",
       "          1.4744591e-03,  2.1361527e-03,  2.8783174e-03],\n",
       "        [-3.0750979e-03,  3.0227075e-03,  2.6943702e-03, ...,\n",
       "          1.5144056e-03,  2.3203588e-03,  3.3736196e-03],\n",
       "        [-3.3267655e-03,  3.3184418e-03,  3.0302031e-03, ...,\n",
       "          1.5087611e-03,  2.4791225e-03,  3.7974222e-03]],\n",
       "\n",
       "       [[-3.8165841e-04, -2.6243613e-05, -1.3113992e-04, ...,\n",
       "          3.4032954e-04,  2.2576917e-04, -3.0043759e-04],\n",
       "        [-7.5666863e-04, -3.5306824e-05, -2.9468647e-04, ...,\n",
       "          4.6486451e-04, -1.6606235e-04, -1.4614918e-04],\n",
       "        [-1.0417177e-03,  9.9706202e-05, -6.9021469e-04, ...,\n",
       "          4.0617323e-04, -5.6423730e-04, -1.7856140e-04],\n",
       "        ...,\n",
       "        [-2.8900858e-03,  2.6896684e-03,  2.8902476e-03, ...,\n",
       "          1.5360995e-03,  1.9703549e-03,  3.2826110e-03],\n",
       "        [-3.1498200e-03,  3.0131340e-03,  3.2077916e-03, ...,\n",
       "          1.4966220e-03,  2.2145454e-03,  3.6544374e-03],\n",
       "        [-3.3545964e-03,  3.2705115e-03,  3.4904040e-03, ...,\n",
       "          1.4425551e-03,  2.4173744e-03,  3.9780452e-03]],\n",
       "\n",
       "       [[-3.8165841e-04, -2.6243613e-05, -1.3113992e-04, ...,\n",
       "          3.4032954e-04,  2.2576917e-04, -3.0043759e-04],\n",
       "        [-4.1582709e-04,  6.4643558e-05, -4.0600286e-04, ...,\n",
       "          8.0849323e-04,  5.8201043e-04, -9.2261913e-04],\n",
       "        [-2.3381662e-04,  6.2969525e-04, -3.7438315e-04, ...,\n",
       "          1.0991750e-03,  8.7047892e-04, -9.6939912e-04],\n",
       "        ...,\n",
       "        [-3.1319063e-03,  3.8020292e-03,  3.5071608e-03, ...,\n",
       "          1.7045432e-03,  3.1529870e-03,  4.0325779e-03],\n",
       "        [-3.3200695e-03,  3.9089383e-03,  3.7949588e-03, ...,\n",
       "          1.6017348e-03,  3.1975645e-03,  4.2999531e-03],\n",
       "        [-3.4612145e-03,  3.9854348e-03,  4.0335981e-03, ...,\n",
       "          1.5064385e-03,  3.2295838e-03,  4.5311959e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-3.8165841e-04, -2.6243613e-05, -1.3113992e-04, ...,\n",
       "          3.4032954e-04,  2.2576917e-04, -3.0043759e-04],\n",
       "        [-3.2934497e-04,  3.6936594e-04, -2.4266508e-04, ...,\n",
       "          3.4116115e-04,  5.2509684e-04, -2.1182832e-04],\n",
       "        [-1.3824637e-04,  4.3931641e-04, -3.2381364e-04, ...,\n",
       "          1.3995363e-04,  7.9618808e-04, -1.9539752e-04],\n",
       "        ...,\n",
       "        [-2.0254541e-03,  2.9378908e-03,  2.7225786e-03, ...,\n",
       "          1.6244230e-03,  1.8623674e-03,  3.3864232e-03],\n",
       "        [-2.4199698e-03,  3.2537370e-03,  3.0555932e-03, ...,\n",
       "          1.6404659e-03,  2.0967140e-03,  3.7380953e-03],\n",
       "        [-2.7477187e-03,  3.5042735e-03,  3.3529017e-03, ...,\n",
       "          1.6205359e-03,  2.2979216e-03,  4.0403525e-03]],\n",
       "\n",
       "       [[-3.8165841e-04, -2.6243613e-05, -1.3113992e-04, ...,\n",
       "          3.4032954e-04,  2.2576917e-04, -3.0043759e-04],\n",
       "        [-6.4561813e-04, -1.3225956e-04, -1.2382490e-04, ...,\n",
       "          5.3526380e-04,  4.4657625e-04, -6.9981924e-04],\n",
       "        [-9.1111806e-04, -1.4856708e-04, -1.0335677e-04, ...,\n",
       "          2.8341878e-04,  8.0957590e-04, -7.8624673e-04],\n",
       "        ...,\n",
       "        [-2.8371471e-03,  2.8899428e-03,  3.1833132e-03, ...,\n",
       "          1.5209360e-03,  1.8879076e-03,  2.9500329e-03],\n",
       "        [-3.1365647e-03,  3.1609568e-03,  3.5069159e-03, ...,\n",
       "          1.5565747e-03,  2.1663532e-03,  3.3811403e-03],\n",
       "        [-3.3772388e-03,  3.3735540e-03,  3.7884705e-03, ...,\n",
       "          1.5563344e-03,  2.4013892e-03,  3.7601800e-03]],\n",
       "\n",
       "       [[-3.8165841e-04, -2.6243613e-05, -1.3113992e-04, ...,\n",
       "          3.4032954e-04,  2.2576917e-04, -3.0043759e-04],\n",
       "        [-5.4875150e-04,  2.6099640e-04, -3.6427571e-04, ...,\n",
       "          3.7335741e-04,  2.6243256e-04, -5.4092926e-04],\n",
       "        [-2.4050620e-04,  1.9793088e-05, -4.3685248e-04, ...,\n",
       "          2.2217323e-04, -1.3919333e-04, -8.0436119e-04],\n",
       "        ...,\n",
       "        [-2.1874767e-03,  3.4194230e-03,  3.2739702e-03, ...,\n",
       "          1.7910721e-03,  2.1650898e-03,  3.3865150e-03],\n",
       "        [-2.4853896e-03,  3.6513908e-03,  3.5666469e-03, ...,\n",
       "          1.7294352e-03,  2.3653917e-03,  3.7278188e-03],\n",
       "        [-2.7351752e-03,  3.8210684e-03,  3.8186461e-03, ...,\n",
       "          1.6480205e-03,  2.5343311e-03,  4.0299478e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
    "# 지금은 동작 원리에 너무 빠져들지 마세요~\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd30071",
   "metadata": {},
   "source": [
    "#### tf.Tensor: shape=(256, 20, 7001)\n",
    "\n",
    "- 256 : dataset.take(1)으로 1개의 데이터셋(shape=(256, 20))을 가져온 결과\n",
    "\n",
    "- 7001 : 7001개의 단어에 대한 예측 확률 값\n",
    "\n",
    "- 20 : ???? \n",
    "> 그렇다면 20은 무엇을 의미할까요? 비밀은 바로 tf.keras.layers.LSTM(hidden_size, return_sequences=True)로 호출한 LSTM 레이어에서 return_sequences=True이라고 지정한 부분에 있습니다. 즉, LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미입니다. 만약 return_sequences=False였다면 LSTM 레이어는 1개의 벡터만 출력했을 것입니다.\n",
    ">\n",
    ">그런데 문제는, 우리의 모델은 입력 데이터의 시퀀스 길이가 얼마인지 모른다는 점입니다. 모델을 만들면서 알려준 적도 없습니다. 그럼 20은 언제 알게된 것일까요? 네, 그렇습니다. 데이터를 입력받으면서 비로소 알게 된 것입니다. 우리 데이터셋의 max_len이 20으로 맞춰져 있었던 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd9a8641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1792256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  7176025   \n",
      "=================================================================\n",
      "Total params: 22,607,961\n",
      "Trainable params: 22,607,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#우리의 모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape를 특정할 수 없는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327a0bd3",
   "metadata": {},
   "source": [
    "### 학습!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "004ff284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 20s 167ms/step - loss: 3.5158\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 16s 169ms/step - loss: 2.8153\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 16s 171ms/step - loss: 2.7290\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 16s 172ms/step - loss: 2.6370\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 16s 173ms/step - loss: 2.5712\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 16s 175ms/step - loss: 2.5242\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 17s 177ms/step - loss: 2.4794\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 2.4356\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 2.3869\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 2.3401\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 2.2947\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 2.2528\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 2.2110\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 2.1712\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 2.1316\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 2.0944\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 2.0552\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 2.0176\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 1.9782\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 1.9395\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 1.9035\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 1.8634\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 1.8281\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 1.7895\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 1.7516\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 1.7148\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 1.6769\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 1.6424\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 1.6061\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 1.5691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4feba36fa0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3509abf",
   "metadata": {},
   "source": [
    "## 3. 모델 평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b87ea",
   "metadata": {},
   "source": [
    "작문의 결과를 알고리즘이 평가하는 것은 무리라고 한다.\n",
    "<br>\n",
    "작문을 시켜보고 직접 평가해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48d5164",
   "metadata": {},
   "source": [
    "### 시작 문장을 전달하면 작문을 진행하는 함수를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d39e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908b128a",
   "metadata": {},
   "source": [
    "#### while문 왜why?\n",
    "\n",
    "1. 학습 단계에서는 준비된 데이터셋(소스 문장과 타겟 문장)을 비교했다.\n",
    "2. 목표는 문장을 생성하는 프로그램을 만드는 것이다. 즉, 테스트 데이터셋을 대상으로 하는 것이 아니다.\n",
    "3. 반복문을 통해, 인자로 받은(혹은 디폴트인 \\<strat>) 값을 기반으로, 다음 값을 예측한다.\n",
    "\n",
    "\n",
    "- while문의 동작 설명은 코드 안의 주석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66296fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> god respected spider spider dull certainly pledge pledge currents autolycus hearing wisdom wisdom coast coast opening opening opening apothecary '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 함수 내부의 생성 문장에 god을 추가해 실행해 본 결과\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> god\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120bc03",
   "metadata": {},
   "source": [
    "# 프로젝트 : '멋진' 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6677697",
   "metadata": {},
   "source": [
    "### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e832f614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "#코퍼스 데이터 경로\n",
    "txt_file_path = path + 'lyricist/data/lyrics/*'\n",
    "\n",
    "#glob으로 파일 위치 리스트 생성\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "l_raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        l_raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기:\", len(l_raw_corpus))\n",
    "print(\"Examples:\\n\", l_raw_corpus[:3])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c363164",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ade97",
   "metadata": {},
   "source": [
    ">```\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    sentence = sentence.strip() \n",
    "    sentence = '\\<start> ' + sentence + ' \\<end>' \n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00a59022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(l_raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "    \n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e9ee3",
   "metadata": {},
   "source": [
    "### 정규표현식으로 정제 후 리스트에 담기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44c32440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_corpus = []\n",
    "\n",
    "for sentence in l_raw_corpus:\n",
    "    \n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "\n",
    "    preprocessed_sentence = preprocess_sentence(sentence) #프리프로세스(..)함수는 예제와 동일하다.\n",
    "    l_corpus.append(preprocessed_sentence)\n",
    "\n",
    "l_corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5181531e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175749"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정규표현식 정제 과정을 거치고 데이터의 양이 약간 감소한 것을 확인 할 수 있다.\n",
    "len(l_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8002fc8",
   "metadata": {},
   "source": [
    "- 비교를 위해 예제의 코드를 카피\n",
    "\n",
    ">```\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=7000, \n",
    "        filters=' ',\n",
    "        oov_token=\"\\<unk>\"\n",
    "    )   \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    ">    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fad38dc",
   "metadata": {},
   "source": [
    "### 토큰화 (텐서 형태로 변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "11ffa2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l_tokenize(a):\n",
    "    l_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  #단어장의 크기, 12000개 이상으로 설정.\n",
    "        filters=' ',\n",
    "        oov_token='<unk>'\n",
    "    )\n",
    "    \n",
    "    l_tokenizer.fit_on_texts(a)\n",
    "    l_tensor = l_tokenizer.texts_to_sequences(a)\n",
    "    \n",
    "    # 토큰화 후 텐서의 형태로 변환하기 전 토큰 갯수가 15를 초과하는 대상을 제외하는 과정\n",
    "    ### print(type(l_tensor)) ###\n",
    "    # 이 단계에서 l_tensor의 데이터 타입을 확인해보면 <class 'list'>로 나온다.\n",
    "    # 일부를 출력해보면 텐서형태로 변환된 것과 유사해 보이지만, 리스트안의 리스트 형태이다.\n",
    "    # 리스트의 리무브 메소드를 for문으로, 각 요소의 길이가 15개를 넘는 경우 리스트에서 제거한다.\n",
    "    \n",
    "    for cut in l_tensor:\n",
    "        if len(cut) > 15:\n",
    "            l_tensor.remove(cut)\n",
    "\n",
    "    l_tensor = tf.keras.preprocessing.sequence.pad_sequences(l_tensor, maxlen=15, padding='post')\n",
    "    \n",
    "    print(l_tensor, l_tokenizer)\n",
    "    return l_tensor, l_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28d4fc",
   "metadata": {},
   "source": [
    "### 위의 tokenize코드(구분을 위해 변수명들에 l_을 붙여줌)에 대한 리뷰\n",
    "\n",
    "> '문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기를 권합니다.'\n",
    "\n",
    "- 세부적 내용을 다 학습하지는 못했지만, 전체적인 흐름에서 막힘 없이 진행되던<br> 이번 익스플로레이션이 가장 난항을 겪은 부분이다.\n",
    "\n",
    "\n",
    "- 원하는 결과를 얻어내기까지 겪은 시행착오는 다음과 같다.\n",
    "    1. 가장 먼저 떠올린 방법은 정규표현식의 코드를 건드려보는 것이었다.\n",
    "\n",
    "        - 이 경우 정규표현식으로 정제하는 대상 데이터가 '문자열 타입이기 때문'에 <br>구현이 어렵다는 것을 바로 파악할 수 있었다.      \n",
    "    2. 다음은 pad_sequences 메소드를 자세히 알아보는 것이었다.\n",
    "\n",
    "        - maxlen 인자값을 받아 길이를 제한할 수 있다는 것을 알게 되었다.\n",
    "            \n",
    "        - 하지만 적용해보니 기대와 달리 데이터 shape의 열 길이가 줄어들지 않았다.\n",
    "        \n",
    "        - 그런데 15개 보다 긴 문장이 없었기 때문에 한참을 생각하게 되었다.\n",
    "        \n",
    "        - 각 단계에서의 shape, len, type등을 출력해보며 알게 된 사실은\n",
    "            <br> 'maxlen'은 이미 텐서화 된 데이터의 최대 길이만 제한하는 파라미터라는 것이다.        \n",
    "            \n",
    "    3. 위의 착오를 겪으며 함수 안에 for문을 작성해주었다. 설명은 코드에 포함된 주석으로 대체.\n",
    "   \n",
    "    4. 마지막 문제는 for문을 넣으며 maxlen을 제거해서 발생했다.\n",
    "        - l_tensor의 shape을 확인해보니 열 길이가 확실히 줄어들었다.\n",
    "        - 그런데 행 길이가 이상했다. 결과는 아래와 같았다.\n",
    "        > print(l_tensor.shape)\n",
    "        > \n",
    "        >(159272, 347)\n",
    "        - 이유는 remove 메소드의 특징에 있었다.\n",
    "        - remove메소드는 리스트의 요소들을 제거하지만, 각 요소들의 자리값은 남아있다.\n",
    "        - l_tensor은 리스트 안에 요소로 리스트가 들어있는 2차원 리스트이다.\n",
    "        - for문을 통해 길이 15이상의 요소 리스트는 remove된다.\n",
    "        - *정확한 이해인지는 모르겠지만* 비어있는 자리값들은 바로 앞 리스트에 합쳐진다. <br>아마 시퀀스 객체의 연속성을 유지하기 위한 방식이 아닐까 추측해보았다.\n",
    "        - 결과적으로 347이라는 길이는 아래와 같이 추측된다.\n",
    "        > 15 이하인 len(tensor)[n] 과\n",
    "        >\n",
    "        > 16 이상인 len(tensor)[n+1] + len(tensor)[n+2] + .. + len(tensor)[n+k]\n",
    "        > \n",
    "        > 의 길이가 최대인 경우\n",
    "        - 그리고 padding=post로 인해 모든 요소의 뒤에 최대 345개의 0이 포함된 텐서가 생성되었다.\n",
    "\n",
    "- maxlen=15 를 다시 넣어준 결과 원하던 결과물을 확인할 수 있었다.\n",
    "\n",
    "*최종적으로..*\n",
    ">  1. 길이값이 15가 넘어 제거된 요소들의 길이가 앞 요소에 더해짐 \n",
    ">  2. maxlen = 15에 의해 텐서의 길이가 제한됨 <br>(여기서 제거된 시퀀스의 뒷부분은 전부 토큰 인덱스 0으로의미가 없는 값임)\n",
    ">  3. 토큰 갯수 15개 이상인 데이터를 제외한 데이터만 텐서로 반환됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4b39406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...   43    3    0]\n",
      " ...\n",
      " [   2  261  200 ...   12    3    0]\n",
      " [  37   15 9049 ...  877  647    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f66b26a5340>\n"
     ]
    }
   ],
   "source": [
    "# 2로 시작하고 있으니 정상적으로 진행되고 있는 것 같다.\n",
    "l_tensor, l_tokenizer = l_tokenize(l_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a3b76",
   "metadata": {},
   "source": [
    "### 텐서 데이터를 객체화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e519a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#소스 문장과 타겟 문장 생성\n",
    "src_input = l_tensor[:, :-1]\n",
    "tgt_input = l_tensor[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "14bf3520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0]\n",
      " [   2   17 2639  873    4    8   11 6043    6  329    3    0    0    0]\n",
      " [   2   36    7   37   15  164  282   28  299    4   47    7   43    3]\n",
      " [   2   11  354   25   42    3    0    0    0    0    0    0    0    0]\n",
      " [   2    6 3604    4    6 2265    3    0    0    0    0    0    0    0]]\n",
      "[[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0]\n",
      " [  17 2639  873    4    8   11 6043    6  329    3    0    0    0    0]\n",
      " [  36    7   37   15  164  282   28  299    4   47    7   43    3    0]\n",
      " [  11  354   25   42    3    0    0    0    0    0    0    0    0    0]\n",
      " [   6 3604    4    6 2265    3    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "#타겟 문장이 2로 시작하지 않고 한 칸씩 밀려있는 것을 확인\n",
    "print(src_input[0:5])\n",
    "print(tgt_input[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8b5eada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 객체 생성은 sklearn의 train_test_split() 메소드로 진행\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d8d08155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (127417, 14)\n",
      "Target Train: (127417, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "16adfcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 정의는 아직 손 댈 영역이 아닌 것 같다. 그대로 사용하자\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(l_tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "959b30be",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2bc418c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 batch 만큼의 임시 모델을 확인해보자\n",
    "for src_sample, tgt_sample in dataset.take(1): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "277a01f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 20, 12001), dtype=float32, numpy=\n",
       "array([[[ 4.22118959e-04,  2.94097612e-04, -1.57962917e-04, ...,\n",
       "          1.80483374e-04, -1.41656012e-04,  1.04516999e-04],\n",
       "        [ 6.67420274e-04,  3.47259192e-04, -1.23224178e-04, ...,\n",
       "          2.77022627e-04, -3.84744722e-04,  3.65340442e-04],\n",
       "        [ 6.16616104e-04,  2.55826395e-04, -2.04375130e-04, ...,\n",
       "          3.15672834e-04, -4.39194526e-04,  5.73804893e-04],\n",
       "        ...,\n",
       "        [ 1.84376759e-03,  3.39808920e-03,  3.44194268e-04, ...,\n",
       "         -1.39116088e-03, -2.68435944e-03,  2.99307867e-04],\n",
       "        [ 1.77112559e-03,  3.54930805e-03,  4.34379675e-04, ...,\n",
       "         -1.58158015e-03, -2.75665359e-03,  2.30596735e-04],\n",
       "        [ 1.68531516e-03,  3.67000024e-03,  5.22705377e-04, ...,\n",
       "         -1.74139964e-03, -2.81358347e-03,  1.69815103e-04]],\n",
       "\n",
       "       [[ 4.22118959e-04,  2.94097612e-04, -1.57962917e-04, ...,\n",
       "          1.80483374e-04, -1.41656012e-04,  1.04516999e-04],\n",
       "        [ 7.09087064e-04,  3.78239405e-04, -4.01817524e-05, ...,\n",
       "          5.35913743e-04, -4.24607599e-04,  3.11463868e-04],\n",
       "        [ 6.98177319e-04,  2.87371106e-04,  2.89740681e-04, ...,\n",
       "          4.30059212e-04, -7.30280532e-04,  3.72647191e-04],\n",
       "        ...,\n",
       "        [ 1.76439004e-03,  1.68180140e-03, -1.84080709e-04, ...,\n",
       "         -1.30041386e-03, -1.49127957e-03,  2.48326221e-04],\n",
       "        [ 1.71895442e-03,  1.99715467e-03, -7.98630936e-05, ...,\n",
       "         -1.48967735e-03, -1.80718431e-03,  2.13719119e-04],\n",
       "        [ 1.66962901e-03,  2.28867447e-03,  3.07178489e-05, ...,\n",
       "         -1.65526941e-03, -2.07380904e-03,  1.74682238e-04]],\n",
       "\n",
       "       [[ 4.22118959e-04,  2.94097612e-04, -1.57962917e-04, ...,\n",
       "          1.80483374e-04, -1.41656012e-04,  1.04516999e-04],\n",
       "        [ 5.80369611e-04,  2.64753617e-04, -1.44958060e-04, ...,\n",
       "          2.82132911e-04, -3.23693996e-04,  1.09334862e-04],\n",
       "        [ 4.46620834e-04,  3.37724836e-04, -1.11264220e-04, ...,\n",
       "          1.80288946e-04, -2.57807726e-04,  4.63988428e-04],\n",
       "        ...,\n",
       "        [ 1.44244242e-03,  2.63698236e-03,  2.03301985e-04, ...,\n",
       "         -1.79495732e-03, -2.59220693e-03, -4.11955261e-05],\n",
       "        [ 1.45312271e-03,  2.85350764e-03,  3.21150670e-04, ...,\n",
       "         -1.93270715e-03, -2.75059044e-03, -4.52535460e-05],\n",
       "        [ 1.43808138e-03,  3.04229697e-03,  4.31020773e-04, ...,\n",
       "         -2.04462255e-03, -2.86282483e-03, -5.27608099e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 4.22118959e-04,  2.94097612e-04, -1.57962917e-04, ...,\n",
       "          1.80483374e-04, -1.41656012e-04,  1.04516999e-04],\n",
       "        [ 7.02836376e-04,  4.66504891e-04, -1.75343710e-04, ...,\n",
       "          2.54450308e-04, -2.71811121e-04,  1.76379704e-04],\n",
       "        [ 1.01730227e-03,  5.80021879e-04, -5.05869539e-05, ...,\n",
       "          3.54319025e-04, -5.17514476e-04,  4.48710809e-04],\n",
       "        ...,\n",
       "        [ 1.29373011e-03,  3.25469929e-03,  5.60793560e-04, ...,\n",
       "         -1.94827316e-03, -2.41120206e-03,  6.85211271e-05],\n",
       "        [ 1.26434804e-03,  3.42101511e-03,  6.16124191e-04, ...,\n",
       "         -2.05626618e-03, -2.51103332e-03,  4.66336278e-05],\n",
       "        [ 1.22516882e-03,  3.55775515e-03,  6.74620387e-04, ...,\n",
       "         -2.14323797e-03, -2.59700511e-03,  2.47135631e-05]],\n",
       "\n",
       "       [[ 4.22118959e-04,  2.94097612e-04, -1.57962917e-04, ...,\n",
       "          1.80483374e-04, -1.41656012e-04,  1.04516999e-04],\n",
       "        [ 5.16931235e-04,  4.99251415e-04, -2.90843949e-04, ...,\n",
       "          2.39470013e-04, -2.85459013e-04,  2.96113401e-04],\n",
       "        [ 7.59374641e-04,  6.85721228e-04, -2.46608979e-04, ...,\n",
       "          5.36638836e-04, -3.68870678e-04,  4.56351030e-04],\n",
       "        ...,\n",
       "        [ 2.20802473e-03,  2.32805288e-03,  1.05782077e-04, ...,\n",
       "         -8.60096479e-04, -2.78713275e-03,  1.51230313e-04],\n",
       "        [ 2.15972867e-03,  2.62378342e-03,  1.99294111e-04, ...,\n",
       "         -1.13989296e-03, -2.89081736e-03,  9.53606650e-05],\n",
       "        [ 2.08601472e-03,  2.87875673e-03,  2.94548576e-04, ...,\n",
       "         -1.37785263e-03, -2.96648010e-03,  4.75478919e-05]],\n",
       "\n",
       "       [[ 4.22118959e-04,  2.94097612e-04, -1.57962917e-04, ...,\n",
       "          1.80483374e-04, -1.41656012e-04,  1.04516999e-04],\n",
       "        [ 8.52809928e-04,  5.51713514e-04,  1.34467855e-05, ...,\n",
       "          3.29397561e-04, -1.93666947e-05,  1.10105895e-04],\n",
       "        [ 1.21030351e-03,  7.33349589e-04,  2.47884629e-04, ...,\n",
       "          6.59959565e-04,  5.77661922e-05,  1.80216070e-04],\n",
       "        ...,\n",
       "        [ 1.57686300e-03,  2.89972336e-03,  5.43449190e-04, ...,\n",
       "         -1.68138696e-03, -2.74547981e-03,  6.07888614e-05],\n",
       "        [ 1.53607293e-03,  3.11684120e-03,  5.93481178e-04, ...,\n",
       "         -1.82210177e-03, -2.82274769e-03,  6.64699619e-05],\n",
       "        [ 1.48299197e-03,  3.29974759e-03,  6.45187218e-04, ...,\n",
       "         -1.94053736e-03, -2.87797931e-03,  6.43551248e-05]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "58df6327",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c34406",
   "metadata": {},
   "source": [
    "### 학습!\n",
    "#### 그리고 두 번째 난관이었다고 한다.\n",
    "\n",
    "- 예제에서 사용한 학습 모델은 아래와 같았다\n",
    "> dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset\n",
    "\n",
    "\n",
    "- 문제는 프로젝트 실습의 경우 사이킷런으로 스플릿 한 데이터를 바로 학습시킨다는 것이었다.\n",
    "\n",
    "\n",
    "- 그 자체는 어려울 것 없이 오히려 익숙했지만, 기본적으로 주어진 코드가 문제였다\n",
    "> enc_train, enc_val, dec_train, dec_val = \n",
    "\n",
    "\n",
    "- 문장을 생성하는 모델은 test데이터에 대한 검증이 아니다.\n",
    "\n",
    "\n",
    "- 그리고 총체적인 기반지식의 부족.\n",
    "    - 테스트 데이터에 해당하는 분류는 어디에 쓰이는 것인가?\n",
    "    - 밸리데이션 용도로 추정되는데 어떻게 사용하는 것인가?\n",
    "    - train_test_split의 파라미터는 train-Validation 의 경우와 train-test 의 경우 다른가?\n",
    "\n",
    "\n",
    "결과적으로 한 epoch의 학습 당 검증까지 한 번에 가능하다는 (어쩌면 당연한)지식을 습득한 후 해결이 되었으나, 큰 인지부조화를 겪어 해결까지 오랜 시간이 걸렸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10epoch안에 val_loss값을 2.2 수준으로 줄이는 것이 목표.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a49c258",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "996/996 [==============================] - 104s 105ms/step - loss: 1.3549 - val_loss: 2.2888\n",
      "Epoch 2/10\n",
      "996/996 [==============================] - 105s 105ms/step - loss: 1.2003 - val_loss: 2.0331\n",
      "Epoch 3/10\n",
      "996/996 [==============================] - 105s 105ms/step - loss: 1.1250 - val_loss: 1.8469\n",
      "Epoch 4/10\n",
      "996/996 [==============================] - 104s 104ms/step - loss: 1.0720 - val_loss: 1.7044\n",
      "Epoch 5/10\n",
      "996/996 [==============================] - 104s 105ms/step - loss: 1.0304 - val_loss: 1.5921\n",
      "Epoch 6/10\n",
      "996/996 [==============================] - 105s 105ms/step - loss: 0.9975 - val_loss: 1.5087\n",
      "Epoch 7/10\n",
      "996/996 [==============================] - 104s 104ms/step - loss: 0.9706 - val_loss: 1.4450\n",
      "Epoch 8/10\n",
      "996/996 [==============================] - 104s 104ms/step - loss: 0.9500 - val_loss: 1.3998\n",
      "Epoch 9/10\n",
      "996/996 [==============================] - 104s 105ms/step - loss: 0.9346 - val_loss: 1.3677\n",
      "Epoch 10/10\n",
      "996/996 [==============================] - 105s 105ms/step - loss: 0.9224 - val_loss: 1.3454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f668908a850>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예제에 주어진 하이퍼파라미터들을 그대로 사용한 결과 val_loss가 2.5 정도에서 멈췄다.\n",
    "# hidden_size는 일꾼의 수..? 일꾼을 늘려보자.\n",
    "# 또 2.2에 도달하지 못해 batch_size를 조절해보았다.\n",
    "# 128에서 256으로 배치사이즈를 키웠을 때, train데이터의 로스율은 상당히 개선되었으나 val_loss는 감소폭이 매우낮았음\n",
    "# 반대로 배치사이즈를 줄여 학습의 간격을 좁혀보았다.\n",
    "# 지속적인 실패에, 노래 가사에는 후렴 반복 등이 많다는 생각이 들었다. 스플릿 과정에 랜덤스테이트를 넣어봄\n",
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model.fit(enc_train, dec_train, epochs=10, batch_size=128, \n",
    "          validation_data = (enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d8c60a",
   "metadata": {},
   "source": [
    "### 한 곡조 뽑아 보거라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7789a71a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> world is gettin ready , everybody s ready , yeah ! <end> '"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, l_tokenizer, init_sentence=\"<start> world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158b7d2",
   "metadata": {},
   "source": [
    "*세상은 준비되고 있고, 모두들 준비됐어, 그래!*\n",
    "\n",
    "만족스러운 무대였습니다. special thx PAPAGO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3d3b4",
   "metadata": {},
   "source": [
    "## 마치며\n",
    "- 비단 익스플로래이션을 진행 할 때가 아니더라도, 의외의 복병에 시간을 빼앗기는 경우가 많다.\n",
    "- 이번 과정에서는 물론 얻은 것이 적지 않다고 생각한다. \n",
    "- 하지만 항상 그럴 수 있는 것은 아니니 삼천포로 빠지지 않고, 맺고 끊는 것을 잘 해야 할 것 같다.\n",
    "- 최종적인 목표..에 어느 것이 더 낫다고 할 수는 없겠지만 이 상태로라면<br> \n",
    "    아이펠 과정을 소화하는 것 자체가 무리일 것 같다.\n",
    "\n",
    "\n",
    "### 기억에 남는 학습 내용\n",
    "- 리스트의 remove 메서드.\n",
    "- train_test_split에서 밸리데이션을 한 번에 돌릴 수 있는 인자.\n",
    "- 스플릿 과정에서 랜덤스테이트를 넣었을 때 나타난 드라마틱한 차이.\n",
    " > 한 인간이 쓰는 단어는 한정적이고, 그 글의 전문성이 낮을 수록 더 심하다.\n",
    " >\n",
    " > 심지어 노래가사가 원본데이터라면... 당연히 섞어줬어야 했다.\n",
    "\n",
    "\n",
    "### 어려웠던 점\n",
    "이번 익스는 이후에 배우게 될 것들이 앞서 나오면서, 우선 넘어가자는 부분이 많았는데\n",
    "그런 부분에서 혼란을 많이 겪은 것 같다. 효율의 측면에서, 문제상황에 직면했을 때, 그 수준이 낮더라도 \n",
    "나만의 해결해 나가는 솔루션을 어느 정도 정형화 할 필요가 느껴졌다. \n",
    "\n",
    "\n",
    "\n",
    "### 추가로 해보고 싶은 점\n",
    "- 지금 당장은 넘모 피곤해서 없다. remove메소드를 시작으로 시퀀스 자료형의 특징들을 공부해봐야겠다.\n",
    "\n",
    "\n",
    "### 총평\n",
    ": 느무..느무느무... 졸립니다..\n",
    "아이펠 화이팅.. 나.. 화이팅...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa94464d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
